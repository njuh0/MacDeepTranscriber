//
//  TranscriptionsView.swift
//  Audio Study
//
//  Created on 21.06.2025.
//

import SwiftUI
import Foundation

// MARK: - Context Window Limits
private func getContextWindowLimit(for model: AIModel) -> Int {
    switch model {
    case .glm4Flash:
        return 8192
    case .gemini2Flash:
        return 1_000_000
    }
}

// MARK: - Output Token Limits
private func getMaxOutputTokens(for model: AIModel) -> Int {
    switch model {
    case .glm4Flash:
        return 4095
    case .gemini2Flash:
        return 8192
    }
}

// MARK: - Chunk Size Limits
private func getMaxChunkSize(for model: AIModel) -> Int {
    switch model {
    case .glm4Flash:
        return 16_000 // 16k characters for GLM-4-Flash
    case .gemini2Flash:
        return 32_000 // 32k characters for Gemini 2.0 Flash (much larger capacity)
    }
}

struct TranscriptionsView: View {
    @ObservedObject var audioCaptureService: AudioCaptureService
    @State private var showSidebar = true // –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –æ—Ç–∫—Ä—ã—Ç
    @State private var recordingsFolders: [String] = []
    @State private var selectedFolder: String? = nil
    @State private var transcriptions: [String: String] = [:] // engine: transcription
    
    var body: some View {
        HStack(spacing: 0) {
            // –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            VStack(spacing: 20) {
                if let selectedFolder = selectedFolder {
                    // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
                    TranscriptionContentView(
                        folderName: selectedFolder,
                        transcriptions: transcriptions,
                        onDeleteTranscription: deleteTranscription,
                        onUpdateTranscriptions: { updatedTranscriptions in
                            self.transcriptions = updatedTranscriptions
                        }
                    )
                } else {
                    // Placeholder –∫–æ–≥–¥–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ
                    Text("Transcriptions")
                        .font(.largeTitle)
                        .fontWeight(.bold)
                        .padding(.top)
                    
                    if recordingsFolders.isEmpty {
                        Text("No transcriptions available")
                            .font(.title2)
                            .foregroundColor(.secondary)
                        
                        Spacer()
                        
                        // Placeholder content –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
                        VStack(spacing: 16) {
                            Image(systemName: "doc.text")
                                .font(.system(size: 60))
                                .foregroundColor(.secondary)
                            
                            Text("No recordings with transcriptions found")
                                .font(.headline)
                                .foregroundColor(.secondary)
                            
                            Text("Create some recordings with transcriptions to see them here")
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                                .multilineTextAlignment(.center)
                        }
                        
                        Spacer()
                    } else {
                        Text("Select a recording from the sidebar")
                            .font(.title2)
                            .foregroundColor(.secondary)
                        
                        Spacer()
                        
                        // Placeholder content
                        VStack(spacing: 16) {
                            Image(systemName: "doc.text")
                                .font(.system(size: 60))
                                .foregroundColor(.secondary)
                            
                            Text("Your transcriptions will appear here")
                                .font(.headline)
                                .foregroundColor(.secondary)
                            
                            Text("Select a recording from the sidebar to view transcription")
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                        }
                        
                        Spacer()
                    }
                }
            }
            .padding()
            .frame(maxWidth: .infinity, maxHeight: .infinity)
            .background(Color(NSColor.controlBackgroundColor))
            
            // –°–∞–π–¥–±–∞—Ä —Å–ø—Ä–∞–≤–∞
            if showSidebar && !recordingsFolders.isEmpty {
                RightSidebarView(
                    audioCaptureService: audioCaptureService,
                    recordingsFolders: recordingsFolders,
                    selectedFolder: $selectedFolder,
                    loadTranscriptions: loadTranscriptions
                )
                    .frame(width: 250)
                    .transition(.asymmetric(
                        insertion: .move(edge: .trailing),
                        removal: .move(edge: .trailing)
                    ))
            }
        }
        .toolbar {
            if !recordingsFolders.isEmpty {
                ToolbarItem(placement: .primaryAction) {
                    Button(action: {
                        audioCaptureService.openRecordingsFolder()
                    }) {
                        Image(systemName: "folder")
                            .font(.title2)
                    }
                    .help("Open Recordings Folder")
                }
                ToolbarItem(placement: .primaryAction) {
                    Button(action: {
                        withAnimation(.easeInOut(duration: 0.3)) {
                            showSidebar.toggle()
                        }
                    }) {
                        Image(systemName: "sidebar.right")
                            .foregroundColor(.primary)
                    }
                    .help(showSidebar ? "Hide Sidebar" : "Show Sidebar")
                }
            }
        }
        .onAppear {
            loadRecordingsFolders()
        }
    }
    
    private func deleteTranscription(engine: String) {
        guard let selectedFolder = selectedFolder else { return }
        
        Task {
            guard let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else {
                print("Could not get documents directory")
                return
            }
            let folderPath = documentsURL.appendingPathComponent("Recordings").appendingPathComponent(selectedFolder).path
            let fileManager = FileManager.default
            
            do {
                if engine == "Apple Speech" {
                    // –ï—Å–ª–∏ —É–¥–∞–ª—è–µ–º Apple Speech, —É–¥–∞–ª—è–µ–º –≤—Å—é –ø–∞–ø–∫—É –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç AI Enhanced
                    try fileManager.removeItem(atPath: folderPath)
                    print("Deleted entire folder \(folderPath) after removing Apple Speech transcription")
                    
                    // –û–±–Ω–æ–≤–ª—è–µ–º UI –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                    await MainActor.run {
                        let folderName = URL(fileURLWithPath: folderPath).lastPathComponent
                        if let index = self.recordingsFolders.firstIndex(of: folderName) {
                            self.recordingsFolders.remove(at: index)
                        }
                        self.selectedFolder = nil
                        self.transcriptions = [:]
                    }
                } else {
                    // –î–ª—è AI Enhanced —É–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç –∫–ª—é—á –∏–∑ JSON
                    let folderContents = try fileManager.contentsOfDirectory(atPath: folderPath)
                    
                    for file in folderContents {
                        if file.hasSuffix(".json") && !file.contains("recording_info") {
                            let filePath = "\(folderPath)/\(file)"
                            let data = try Data(contentsOf: URL(fileURLWithPath: filePath))
                            
                            if var json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                                if json["aiEnhancedTranscription"] != nil {
                                    json.removeValue(forKey: "aiEnhancedTranscription")
                                    
                                    let updatedData = try JSONSerialization.data(withJSONObject: json, options: .prettyPrinted)
                                    try updatedData.write(to: URL(fileURLWithPath: filePath))
                                    
                                    print("Deleted AI Enhanced transcription from \(file)")
                                    
                                    // –û–±–Ω–æ–≤–ª—è–µ–º UI –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                                    await MainActor.run {
                                        var updatedTranscriptions = self.transcriptions
                                        updatedTranscriptions.removeValue(forKey: "AI Enhanced")
                                        self.transcriptions = updatedTranscriptions
                                    }
                                    break
                                }
                            }
                        }
                    }
                }
            } catch {
                print("Error deleting transcription: \(error)")
            }
        }
    }
    
    private func loadTranscriptions(for folderName: String) {
        print("Loading transcriptions for folder: \(folderName)")
        
        // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        transcriptions = [:]
        
        Task {
            guard let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else {
                print("Could not get documents directory")
                return
            }
            let folderPath = documentsURL.appendingPathComponent("Recordings").appendingPathComponent(folderName).path
            let fileManager = FileManager.default
            
            var newTranscriptions: [String: String] = [:]
            
            do {
                print("Checking folder path: \(folderPath)")
                
                guard fileManager.fileExists(atPath: folderPath) else {
                    print("Folder does not exist: \(folderPath)")
                    await MainActor.run {
                        self.transcriptions = [:]
                    }
                    return
                }
                
                let folderContents = try fileManager.contentsOfDirectory(atPath: folderPath)
                print("Found files in folder: \(folderContents)")
                
                for file in folderContents {
                    if file.hasSuffix(".json") && !file.contains("recording_info") {
                        let filePath = "\(folderPath)/\(file)"
                        print("Reading JSON file: \(filePath)")
                        
                        let data = try Data(contentsOf: URL(fileURLWithPath: filePath))
                        
                        if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                            print("JSON structure keys: \(json.keys)")
                            
                            // Parse Apple Speech transcriptions
                            if let appleSpeechTranscriptions = json["appleSpeechTranscriptions"] as? [[String: Any]] {
                                let transcriptions = appleSpeechTranscriptions.compactMap { item in
                                    item["transcription"] as? String
                                }.joined(separator: "\n\n")
                                
                                if !transcriptions.isEmpty {
                                    newTranscriptions["Apple Speech"] = transcriptions
                                    print("Added Apple Speech transcriptions: \(appleSpeechTranscriptions.count) items")
                                }
                            }
                            
                            // Parse AI Enhanced transcription
                            if let aiEnhancedTranscription = json["aiEnhancedTranscription"] as? String {
                                if !aiEnhancedTranscription.isEmpty {
                                    newTranscriptions["AI Enhanced"] = aiEnhancedTranscription
                                    print("Added AI Enhanced transcription")
                                }
                            }
                        } else {
                            print("Failed to parse JSON from file: \(file)")
                        }
                    }
                }
                
                print("Total transcriptions loaded: \(newTranscriptions.count)")
                
                // –û–±–Ω–æ–≤–ª—è–µ–º UI –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                await MainActor.run {
                    self.transcriptions = newTranscriptions
                }
            } catch {
                print("Error loading transcriptions: \(error)")
                await MainActor.run {
                    self.transcriptions = [:]
                }
            }
        }
    }
    
    private func loadRecordingsFolders() {
        DispatchQueue.global(qos: .userInitiated).async {
            guard let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else {
                print("Could not get documents directory")
                return
            }
            let recordingsPath = documentsURL.appendingPathComponent("Recordings").path
            let fileManager = FileManager.default
            
            guard fileManager.fileExists(atPath: recordingsPath) else {
                print("Recordings folder does not exist at: \(recordingsPath)")
                return
            }
            
            do {
                let folderContents = try fileManager.contentsOfDirectory(atPath: recordingsPath)
                var foldersWithJSON: [String] = []
                
                for item in folderContents {
                    let itemPath = "\(recordingsPath)/\(item)"
                    var isDirectory: ObjCBool = false
                    
                    if fileManager.fileExists(atPath: itemPath, isDirectory: &isDirectory) && isDirectory.boolValue {
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ JSON —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
                        let folderContents = try fileManager.contentsOfDirectory(atPath: itemPath)
                        if folderContents.contains(where: { $0.hasSuffix(".json") }) {
                            foldersWithJSON.append(item)
                        }
                    }
                }
                
                DispatchQueue.main.async {
                    self.recordingsFolders = foldersWithJSON.sorted()
                }
            } catch {
                print("Error loading recordings folders: \(error)")
            }
        }
    }
}

struct RightSidebarView: View {
    @ObservedObject var audioCaptureService: AudioCaptureService
    let recordingsFolders: [String]
    @Binding var selectedFolder: String?
    let loadTranscriptions: (String) -> Void
    
    var body: some View {
        VStack(alignment: .leading, spacing: 0) {
            
            // –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ø–∏—Å–µ–π
            if !recordingsFolders.isEmpty {
                Divider()
                
                Text("Recordings")
                    .font(.headline)
                    .padding(.horizontal)
                    .padding(.top, 15)
                    .padding(.bottom, 10)
                
                // –°–ø–∏—Å–æ–∫ –ø–∞–ø–æ–∫ —Å –∑–∞–ø–∏—Å—è–º–∏
                List(recordingsFolders, id: \.self) { folder in
                    HStack {
                        Image(systemName: "doc.text")
                            .foregroundColor(.secondary)
                        Text(folder)
                            .font(.system(size: 14))
                            .foregroundColor(.primary)
                        Spacer()
                    }
                    .padding(.vertical, 6)
                    .padding(.horizontal, 8)
                    .background(
                        selectedFolder == folder ? 
                            Color.accentColor.opacity(0.2) : 
                            Color.clear
                    )
                    .cornerRadius(6)
                    .contentShape(Rectangle())
                    .onTapGesture {
                        selectedFolder = folder
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            loadTranscriptions(folder)
                        }
                    }
                }
                .listStyle(PlainListStyle())
            }
        }
        .frame(maxHeight: .infinity)
        .background(Color(NSColor.controlBackgroundColor))
        .overlay(
            Rectangle()
                .frame(width: 1)
                .foregroundColor(Color(NSColor.separatorColor)),
            alignment: .leading
        )
    }
}

struct TranscriptionContentView: View {
    let folderName: String
    let transcriptions: [String: String]
    let onDeleteTranscription: (String) -> Void
    let onUpdateTranscriptions: ([String: String]) -> Void
    @State private var isEnhancing = false
    @State private var enhancedTranscription: String? = nil
    @State private var showEnhancedTranscription = false
    @State private var aiService = UniversalAIChatService()
    @AppStorage("selectedAIModel") private var selectedModel: String = AIModel.glm4Flash.rawValue
    @AppStorage("zhipu_api_key") private var zhipuAPIKey: String = ""
    @AppStorage("google_api_key") private var googleAPIKey: String = ""
    

    
    private var currentModel: AIModel {
        AIModel(rawValue: selectedModel) ?? .glm4Flash
    }
    
    private var currentAPIKey: String {
        switch currentModel.provider {
        case .zhipuAI:
            return zhipuAPIKey
        case .googleAI:
            return googleAPIKey
        }
    }
    
    var body: some View {
        VStack(spacing: 20) {
            // –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∑–∞–ø–∏—Å–∏ –∏ –∫–Ω–æ–ø–∫–æ–π AI Enhancement
            HStack {
                Text(folderName)
                    .font(.largeTitle)
                    .fontWeight(.bold)
                
                Spacer()
                
                // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–Ω–µ AI Enhanced)
                if !transcriptions.isEmpty && !transcriptions.keys.contains("AI Enhanced") {
                    VStack(alignment: .trailing, spacing: 4) {
                        // –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
                        Text("Model: \(currentModel.displayName)")
                            .font(.caption)
                            .foregroundColor(.secondary)
                        
                        Button(action: {
                            enhanceTranscription()
                        }) {
                            HStack(spacing: 6) {
                                if isEnhancing {
                                    ProgressView()
                                        .progressViewStyle(CircularProgressViewStyle(tint: .white))
                                        .scaleEffect(0.5)
                                        .frame(width: 14, height: 14)
                                } else {
                                    Image(systemName: "sparkles")
                                        .font(.system(size: 14))
                                        .frame(width: 14, height: 14)
                                }
                                Text("AI Enhance")
                                    .font(.system(size: 14, weight: .medium))
                            }
                            .padding(.horizontal, 12)
                            .padding(.vertical, 6)
                        }
                        .buttonStyle(.borderedProminent)
                        .disabled(isEnhancing)
                        .help("Enhance transcription quality using \(currentModel.displayName)")
                    }
                } else if transcriptions.keys.contains("AI Enhanced") {
                    VStack(alignment: .trailing, spacing: 4) {
                        // –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
                        Text("Model: \(currentModel.displayName)")
                            .font(.caption)
                            .foregroundColor(.secondary)
                        
                        // –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è
                        Button(action: {
                            enhanceTranscription()
                        }) {
                            HStack(spacing: 6) {
                                if isEnhancing {
                                    ProgressView()
                                        .progressViewStyle(CircularProgressViewStyle(tint: .primary))
                                        .scaleEffect(0.5)
                                        .frame(width: 14, height: 14)
                                } else {
                                    Image(systemName: "arrow.clockwise")
                                        .font(.system(size: 14))
                                        .frame(width: 14, height: 14)
                                }
                                Text("Re-enhance")
                                    .font(.system(size: 14, weight: .medium))
                            }
                            .padding(.horizontal, 12)
                            .padding(.vertical, 6)
                        }
                        .buttonStyle(.bordered)
                        .disabled(isEnhancing)
                        .help("Re-enhance transcription with \(currentModel.displayName)")
                    }
                }
            }
            .padding(.top)
            
            if transcriptions.isEmpty {
                // –ó–∞–≥—Ä—É–∑–∫–∞
                VStack(spacing: 16) {
                    ProgressView()
                        .progressViewStyle(CircularProgressViewStyle())
                    Text("Loading transcription...")
                        .font(.headline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth: .infinity, maxHeight: .infinity)
            } else {
                VStack(spacing: 20) {
                    // Main transcriptions (Apple Speech only)
                    let originalTranscriptions = transcriptions.filter { $0.key != "AI Enhanced" }
                    
                    if originalTranscriptions.count == 1 {
                        // One original transcription (Apple Speech)
                        let (engine, transcription) = originalTranscriptions.first!
                        SingleTranscriptionView(
                            engine: engine, 
                            transcription: transcription,
                            onDelete: { onDeleteTranscription(engine) }
                        )
                    } else if originalTranscriptions.count > 1 {
                        // Multiple original transcriptions (if extended in future)
                        HStack(spacing: 20) {
                            ForEach(originalTranscriptions.sorted(by: { $0.key < $1.key }), id: \.key) { engine, transcription in
                                SingleTranscriptionView(
                                    engine: engine, 
                                    transcription: transcription,
                                    onDelete: { onDeleteTranscription(engine) }
                                )
                                .frame(maxWidth: .infinity)
                            }
                        }
                    }
                    
                    // AI Enhanced transcription below the main ones
                    if let aiEnhanced = transcriptions["AI Enhanced"] {
                        VStack(spacing: 12) {
                            HStack {
                                Text("AI Enhanced Result")
                                    .font(.title2)
                                    .fontWeight(.semibold)
                                Spacer()
                            }
                            
                            SingleTranscriptionView(
                                engine: "AI Enhanced",
                                transcription: aiEnhanced,
                                onDelete: { deleteAIEnhancedTranscription() }
                            )
                        }
                    }
                }
            }
            
            Spacer()
        }
        .padding()
    }
    

    
    private func enhanceTranscription() {
        guard !transcriptions.isEmpty else { return }
        guard !currentAPIKey.isEmpty else {
            print("API key not configured for \(currentModel.displayName)")
            return
        }
        
        print("=== AI Enhancement Started ===")
        print("Folder: \(folderName)")
        print("Session ID: \(UUID().uuidString.prefix(8))")
        print("Selected Model: \(currentModel.displayName) (\(currentModel.rawValue))")
        print("Provider: \(currentModel.provider.displayName)")
        print("Has API Key: \(!currentAPIKey.isEmpty)")
        print("Original transcriptions count: \(transcriptions.filter { $0.key != "AI Enhanced" }.count)")
        
        // Log current model limits
        logModelLimits()
        
        // Uncomment for debugging all models:
        // logAllModelLimits()
        
        isEnhancing = true
        
        Task {
            do {
                // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º AI —Å–µ—Ä–≤–∏—Å —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ –∫–ª—é—á–æ–º
                print("Configuring AI service with model: \(currentModel.displayName)")
                aiService.updateConfiguration(apiKey: currentAPIKey, model: currentModel)
                
                // –°–æ–∑–¥–∞–µ–º prompt –¥–ª—è AI, –∏—Å–∫–ª—é—á–∞—è AI Enhanced –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                let originalTranscriptions = transcriptions.filter { $0.key != "AI Enhanced" }
                let allTranscriptions = originalTranscriptions.map { engine, text in
                    "=== \(engine) ===\n\(text)"
                }.joined(separator: "\n\n")
                
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                let maxChunkSize = getMaxChunkSize(for: currentModel) // –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏
                
                print("Text length: \(allTranscriptions.count) characters, Max chunk size: \(maxChunkSize) characters")
                print("Will use \(allTranscriptions.count <= maxChunkSize ? "single chunk" : "multiple chunks") processing")
                
                let enhancedText: String
                
                if allTranscriptions.count <= maxChunkSize {
                    // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑
                    let rawEnhanced = try await processTranscriptionChunk(allTranscriptions)
                    
                    // –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    let shouldRemoveDuplicates = false // –ò–∑–º–µ–Ω–∏—Ç—å –Ω–∞ true –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    enhancedText = shouldRemoveDuplicates ? removeDuplicateSegments(rawEnhanced) : rawEnhanced
                    
                    if !shouldRemoveDuplicates {
                        print("‚ö†Ô∏è  Duplicate removal is DISABLED for debugging")
                    }
                } else {
                    // –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é
                    let rawEnhanced = try await processLargeTranscription(allTranscriptions, maxChunkSize: maxChunkSize, folderName: folderName)
                    
                    // –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    let shouldRemoveDuplicates = false // –ò–∑–º–µ–Ω–∏—Ç—å –Ω–∞ true –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    enhancedText = shouldRemoveDuplicates ? removeDuplicateSegments(rawEnhanced) : rawEnhanced
                    
                    if !shouldRemoveDuplicates {
                        print("‚ö†Ô∏è  Duplicate removal is DISABLED for debugging")
                    }
                }
                
                // –°–æ—Ö—Ä–∞–Ω—è–µ–º AI Enhanced —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤ JSON
                await saveAIEnhancedTranscription(enhancedText)
                
                print("=== AI Enhancement Completed ===")
                print("Original text length: \(allTranscriptions.count) characters")
                print("Enhanced text length: \(enhancedText.count) characters")
                print("Model used: \(currentModel.displayName)")
                print("Duplicates were processed and removed")
                
                await MainActor.run {
                    self.isEnhancing = false
                }
            } catch {
                print("=== AI Enhancement Failed ===")
                print("Error enhancing transcription with \(currentModel.displayName): \(error)")
                await MainActor.run {
                    self.isEnhancing = false
                }
            }
        }
    }
    
    private func processTranscriptionChunk(_ text: String) async throws -> String {
        let chunkTokens = estimateTokenCount(text)
        let maxContextTokens = getContextWindowLimit(for: currentModel)
        let contextUsagePercent = min(100, Int(Double(chunkTokens) / Double(maxContextTokens) * 100))
        
        print("=== Processing Single Chunk ===")
        print("Chunk length: \(text.count) characters (~\(chunkTokens) tokens)")
        print("Context usage: \(contextUsagePercent)% (\(chunkTokens)/\(maxContextTokens) tokens)")
        print("Detected language pattern: \(detectLanguage(in: text))")
        print("Language will be auto-detected by AI model")
        
        let prompt = """
        I have a speech recognition transcription that contains errors and needs cleaning. Please analyze the language and fix this transcription by:

        1. Automatically detecting the language of the transcription
        2. Correcting obvious spelling mistakes and typos in that language
        3. Fixing punctuation, capitalization, and spacing according to language rules
        4. Removing duplicate sentences, paragraphs or phrases that appear to be recognition errors
        5. Completing cut-off words or sentences that end abruptly
        6. Ensuring proper sentence structure and natural flow for the detected language
        7. Fixing names and technical terms that were misrecognized
        8. Adding missing punctuation marks appropriate for the language
        9. Keeping ALL the original unique content - do not summarize or shorten

        IMPORTANT: 
        - First detect the language, then apply language-specific corrections
        - Return the complete cleaned transcription without duplications
        - Maintain all the original unique information and meaning
        - If text is repeated multiple times, keep only one clean version
        - Complete any sentences that end abruptly or are cut off
        - Ensure natural language flow according to the detected language's grammar rules
        - Preserve the original language - do not translate

        Here is the transcription to clean:

        \(text)

        Please provide the complete cleaned transcription in the same language:
        """
       
        
        return try await aiService.sendMessage(prompt, conversationHistory: [])
    }
    
    private func processLargeTranscription(_ text: String, maxChunkSize: Int, folderName: String) async throws -> String {
        let sessionId = UUID().uuidString.prefix(8)
        let maxContextTokens = getContextWindowLimit(for: currentModel)
        
        print("=== Processing Large Transcription ===")
        print("Session ID: \(sessionId)")
        print("Folder: \(folderName)")
        print("Model: \(currentModel.displayName) (Provider: \(currentModel.provider.displayName))")
        print("Context window limit: \(maxContextTokens) tokens")
        print("Max chunk size: \(maxChunkSize) characters")
        print("Total text length: \(text.count) characters (~\(estimateTokenCount(text)) tokens)")
        print("Detected language pattern: \(detectLanguage(in: text))")
        print("Language will be auto-detected by AI model for each chunk")
        print("Starting conversation with empty history (isolated per recording)")
        
        // –ë–æ–ª–µ–µ —É–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ä–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏ —Å –ø–æ–∏—Å–∫–æ–º –≥—Ä–∞–Ω–∏—Ü
        var chunks: [String] = []
        
        if text.count <= maxChunkSize {
            chunks = [text]
        } else {
            var currentIndex = 0
            
            while currentIndex < text.count {
                let remainingText = String(text[text.index(text.startIndex, offsetBy: currentIndex)...])
                
                if remainingText.count <= maxChunkSize {
                    // –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ–∫
                    chunks.append(remainingText.trimmingCharacters(in: .whitespacesAndNewlines))
                    break
                }
                
                // –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
                let targetChunkSize = maxChunkSize
                let minChunkSize = maxChunkSize / 2  // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                
                // –ò—â–µ–º –≥—Ä–∞–Ω–∏—Ü—É –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç —Å–µ—Ä–µ–¥–∏–Ω—ã –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                let searchStart = max(minChunkSize, targetChunkSize / 2)
                let searchEnd = min(targetChunkSize + 200, remainingText.count)
                
                var bestCutPoint = min(targetChunkSize, remainingText.count)
                
                // –ò—â–µ–º –ª—É—á—à—É—é —Ç–æ—á–∫—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                for i in stride(from: searchEnd, to: searchStart, by: -1) {
                    if i >= remainingText.count { continue }
                    
                    let char = remainingText[remainingText.index(remainingText.startIndex, offsetBy: i)]
                    
                    // 1. –ö–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                    if char == "." || char == "!" || char == "?" {
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞ –µ—Å—Ç—å –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
                        if i + 1 < remainingText.count {
                            let nextChar = remainingText[remainingText.index(remainingText.startIndex, offsetBy: i + 1)]
                            if nextChar == " " || nextChar == "\n" {
                                bestCutPoint = i + 1
                                break
                            }
                        } else {
                            bestCutPoint = i + 1
                            break
                        }
                    }
                    // 2. –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                    else if char == "\n" {
                        bestCutPoint = i + 1
                        break
                    }
                }
                
                // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à—É—é –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏—â–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                if bestCutPoint == min(targetChunkSize, remainingText.count) {
                    for i in stride(from: searchEnd, to: searchStart, by: -1) {
                        if i >= remainingText.count { continue }
                        
                        let char = remainingText[remainingText.index(remainingText.startIndex, offsetBy: i)]
                        
                        // 3. –ó–∞–ø—è—Ç–∞—è –∏–ª–∏ —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π
                        if char == "," || char == ";" {
                            if i + 1 < remainingText.count {
                                let nextChar = remainingText[remainingText.index(remainingText.startIndex, offsetBy: i + 1)]
                                if nextChar == " " {
                                    bestCutPoint = i + 1
                                    break
                                }
                            }
                        }
                    }
                }
                
                // –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –∏—â–µ–º –ª—é–±–æ–π –ø—Ä–æ–±–µ–ª
                if bestCutPoint == min(targetChunkSize, remainingText.count) {
                    for i in stride(from: searchEnd, to: searchStart, by: -1) {
                        if i >= remainingText.count { continue }
                        
                        let char = remainingText[remainingText.index(remainingText.startIndex, offsetBy: i)]
                        
                        // 4. –õ—é–±–æ–π –ø—Ä–æ–±–µ–ª
                        if char == " " {
                            bestCutPoint = i + 1
                            break
                        }
                    }
                }
                
                // –ë–µ—Ä–µ–º –∫—É—Å–æ–∫ –¥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä–µ–∑–∞
                bestCutPoint = min(bestCutPoint, remainingText.count)
                let chunk = String(remainingText.prefix(bestCutPoint)).trimmingCharacters(in: .whitespacesAndNewlines)
                chunks.append(chunk)
                
                currentIndex += bestCutPoint
            }
        }
        
        print("Processing \(chunks.count) chunks for large transcription (sizes: \(chunks.map { $0.count }))")
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        var enhancedChunks: [String] = []
        var conversationHistory: [(String, String)] = [] // (user_message, ai_response)
        
        for (index, chunk) in chunks.enumerated() {
            // –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏
            let historyTokens = estimateTokenCountForHistory(conversationHistory)
            let currentChunkTokens = estimateTokenCount(chunk)
            let promptTokens = estimateTokenCount("""
            I have part \(index + 1) of \(chunks.count) of a speech transcription. Please make MINIMAL corrections only for obvious errors.

            Only fix:
            1. Clear spelling mistakes
            2. Missing spaces between words  
            3. Obvious duplicate words that are recognition errors
            4. Basic punctuation where clearly missing

            DO NOT:
            - Rephrase or rewrite anything
            - Change the speaking style
            - Add words that weren't there
            - Make stylistic changes
            - Summarize or shorten

            Keep this part as close to the original as possible.

            Original part:

            Minimally corrected version:
            """)
            
            let totalRequestTokens = historyTokens + currentChunkTokens + promptTokens
            let contextUsagePercent = min(100, Int(Double(totalRequestTokens) / Double(maxContextTokens) * 100))
            
            // –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏
            let barLength = 20
            let filledLength = Int(Double(barLength) * Double(contextUsagePercent) / 100.0)
            let emptyLength = barLength - filledLength
            let progressBar = String(repeating: "‚ñà", count: filledLength) + String(repeating: "‚ñë", count: emptyLength)
            
            print("[Session \(sessionId)] Processing chunk \(index + 1)/\(chunks.count)")
            print("[Session \(sessionId)] Conversation history: \(conversationHistory.count) interactions (~\(historyTokens) tokens)")
            print("[Session \(sessionId)] Current chunk: \(chunk.count) chars (~\(currentChunkTokens) tokens)")
            print("[Session \(sessionId)] Total request: ~\(totalRequestTokens) tokens")
            print("[Session \(sessionId)] Context window: [\(progressBar)] \(contextUsagePercent)% (\(totalRequestTokens)/\(maxContextTokens))")
            
            if contextUsagePercent >= 80 {
                print("[Session \(sessionId)] ‚ö†Ô∏è  WARNING: Context window is \(contextUsagePercent)% full - approaching limit!")
            } else if contextUsagePercent >= 60 {
                print("[Session \(sessionId)] ‚ÑπÔ∏è  INFO: Context window is \(contextUsagePercent)% full")
            }
            
            let prompt = """
            I have part \(index + 1) of \(chunks.count) of a speech transcription. Please make corrections for errors while preserving the original meaning and language.

            Please:
            1. Automatically detect the language of this text part
            2. Fix clear spelling mistakes and typos in that language
            3. Correct missing spaces between words  
            4. Remove obvious duplicate words or phrases that are recognition errors
            5. Add basic punctuation where clearly missing according to language rules
            6. Complete cut-off words or incomplete sentences
            7. Fix names and technical terms that were misrecognized

            IMPORTANT RULES:
            - Keep the original content, meaning, and language intact
            - Do NOT translate to another language
            - Do NOT rephrase or rewrite in your own words
            - Do NOT change the speaking style or add explanations
            - Do NOT summarize or shorten the content
            - If this part seems to repeat content from previous parts, still process it (duplicates will be handled separately)
            - Complete any sentences that end abruptly
            - Apply language-specific grammar and punctuation rules

            Original part \(index + 1)/\(chunks.count):

            \(chunk)

            Corrected version (same language):
            """
            
            // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è AI —Å–µ—Ä–≤–∏—Å–∞
            var chatHistory = conversationHistory.flatMap { userMsg, aiResponse in
                [
                    ChatMessage(content: userMsg, isFromUser: true),
                    ChatMessage(content: aiResponse, isFromUser: false)
                ]
            }
            
            // –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–æ, —Å–æ–∫—Ä–∞—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if contextUsagePercent >= 90 {
                let maxHistoryMessages = max(2, chatHistory.count / 2) // –û—Å—Ç–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º—É–º 2 —Å–æ–æ–±—â–µ–Ω–∏—è
                chatHistory = Array(chatHistory.suffix(maxHistoryMessages))
                let reducedTokens = estimateTokenCountForHistory(conversationHistory.suffix(maxHistoryMessages / 2))
                print("[Session \(sessionId)] üîÑ Context window at \(contextUsagePercent)%, reducing history to \(maxHistoryMessages) messages (~\(reducedTokens) tokens)")
            }
            
            print("[Session \(sessionId)] Sending chunk \(index + 1) to AI with \(chatHistory.count) messages in conversation history")
            
            let enhancedChunk = try await aiService.sendMessage(prompt, conversationHistory: chatHistory)
            let cleanedChunk = enhancedChunk.trimmingCharacters(in: .whitespacesAndNewlines)
            enhancedChunks.append(cleanedChunk)
            
            // –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
            conversationHistory.append((prompt, cleanedChunk))
            
            print("[Session \(sessionId)] Chunk \(index + 1) processed. Enhanced length: \(cleanedChunk.count) characters")
            print("[Session \(sessionId)] Total conversation history: \(conversationHistory.count) interactions")
            
            // –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            try await Task.sleep(nanoseconds: 500_000_000) // 0.5 —Å–µ–∫—É–Ω–¥—ã
        }
        
        // –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ —Å –ø—Ä–æ–±–µ–ª–æ–º, —É–±–∏—Ä–∞—è –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        let result = enhancedChunks.joined(separator: " ")
        let finalResult = result.replacingOccurrences(of: "  ", with: " ").trimmingCharacters(in: .whitespacesAndNewlines)
        
        let maxHistoryTokens = estimateTokenCountForHistory(conversationHistory)
        let maxContextUsagePercent = min(100, Int(Double(maxHistoryTokens) / Double(maxContextTokens) * 100))
        
        // –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        let barLength = 20
        let filledLength = Int(Double(barLength) * Double(maxContextUsagePercent) / 100.0)
        let emptyLength = barLength - filledLength
        let maxProgressBar = String(repeating: "‚ñà", count: filledLength) + String(repeating: "‚ñë", count: emptyLength)
        
        print("=== Large Transcription Processing Complete ===")
        print("Session ID: \(sessionId)")
        print("Final enhanced text length: \(finalResult.count) characters")
        print("Total chunks processed: \(enhancedChunks.count)")
        print("Final conversation history size: \(conversationHistory.count) interactions (~\(maxHistoryTokens) tokens)")
        print("Max context window: [\(maxProgressBar)] \(maxContextUsagePercent)% (\(maxHistoryTokens)/\(maxContextTokens))")
        
        return finalResult
    }
    
    private func saveAIEnhancedTranscription(_ enhancedText: String) async {
        guard let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else {
            print("Could not get documents directory")
            return
        }
        let folderPath = documentsURL.appendingPathComponent("Recordings").appendingPathComponent(folderName).path
        let fileManager = FileManager.default
        
        do {
            let folderContents = try fileManager.contentsOfDirectory(atPath: folderPath)
            
            for file in folderContents {
                if file.hasSuffix(".json") && !file.contains("recording_info") {
                    let filePath = "\(folderPath)/\(file)"
                    let data = try Data(contentsOf: URL(fileURLWithPath: filePath))
                    
                    if var json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                        // –î–æ–±–∞–≤–ª—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º AI Enhanced —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
                        json["aiEnhancedTranscription"] = enhancedText
                        
                        let updatedData = try JSONSerialization.data(withJSONObject: json, options: .prettyPrinted)
                        try updatedData.write(to: URL(fileURLWithPath: filePath))
                        
                        print("Saved AI Enhanced transcription to \(file)")
                        
                        // –û–±–Ω–æ–≤–ª—è–µ–º UI –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                        await MainActor.run {
                            var updatedTranscriptions = self.transcriptions
                            updatedTranscriptions["AI Enhanced"] = enhancedText
                            self.onUpdateTranscriptions(updatedTranscriptions)
                        }
                        break
                    }
                }
            }
        } catch {
            print("Error saving AI Enhanced transcription: \(error)")
        }
    }
    
    private func deleteAIEnhancedTranscription() {
        guard let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else {
            print("Could not get documents directory")
            return
        }
        let folderPath = documentsURL.appendingPathComponent("Recordings").appendingPathComponent(folderName).path
        let fileManager = FileManager.default
        
        Task {
            do {
                let folderContents = try fileManager.contentsOfDirectory(atPath: folderPath)
                
                for file in folderContents {
                    if file.hasSuffix(".json") && !file.contains("recording_info") {
                        let filePath = "\(folderPath)/\(file)"
                        let data = try Data(contentsOf: URL(fileURLWithPath: filePath))
                        
                        if var json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                            if json["aiEnhancedTranscription"] != nil {
                                json.removeValue(forKey: "aiEnhancedTranscription")
                                
                                let updatedData = try JSONSerialization.data(withJSONObject: json, options: .prettyPrinted)
                                try updatedData.write(to: URL(fileURLWithPath: filePath))
                                
                                print("Deleted AI Enhanced transcription from \(file)")
                                
                                // –û–±–Ω–æ–≤–ª—è–µ–º UI –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                                await MainActor.run {
                                    var updatedTranscriptions = self.transcriptions
                                    updatedTranscriptions.removeValue(forKey: "AI Enhanced")
                                    self.onUpdateTranscriptions(updatedTranscriptions)
                                }
                                break
                            }
                        }
                    }
                }
            } catch {
                print("Error deleting AI Enhanced transcription: \(error)")
            }
        }
    }
    
    // MARK: - Language Detection Helper
    
    private func detectLanguage(in text: String) -> String {
        let sampleText = text.prefix(200) // –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏
        if sampleText.range(of: "[–∞-—è—ë—ñ]", options: [.regularExpression, .caseInsensitive]) != nil {
            return "Cyrillic (Russian/Ukrainian/etc.)"
        } else if sampleText.range(of: "[\\u4e00-\\u9fff\\u3040-\\u309f\\u30a0-\\u30ff]", options: [.regularExpression]) != nil {
            return "CJK (Chinese/Japanese/Korean)"
        } else if sampleText.range(of: "[\\u0600-\\u06ff\\u0750-\\u077f]", options: [.regularExpression]) != nil {
            return "Arabic"
        } else if sampleText.range(of: "[\\u0590-\\u05ff]", options: [.regularExpression]) != nil {
            return "Hebrew"
        } else if sampleText.range(of: "[√†√°√¢√£√§√•√¶√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√∏√π√∫√ª√º√Ω]", options: [.regularExpression, .caseInsensitive]) != nil {
            return "Romance Language (French/Spanish/Italian/etc.)"
        } else if sampleText.range(of: "[√§√∂√º√ü]", options: [.regularExpression, .caseInsensitive]) != nil {
            return "German"
        } else if sampleText.range(of: "[a-z]", options: [.regularExpression, .caseInsensitive]) != nil {
            return "Latin-based (English/etc.)"
        } else {
            return "Unknown/Mixed"
        }
    }
    
    // MARK: - Debug Functions
    
    func logAllModelLimits() {
        print("=== All AI Model Limits ===")
        
        for model in AIModel.allCases {
            let contextLimit = getContextWindowLimit(for: model)
            let outputLimit = getMaxOutputTokens(for: model)
            let chunkSize = getMaxChunkSize(for: model)
            
            print("ü§ñ \(model.displayName) (\(model.provider.displayName)):")
            print("   Context: \(contextLimit) tokens | Output: \(outputLimit) tokens | Chunk: \(chunkSize) chars")
        }
        
        print("===============================")
    }
    
    // MARK: - Duplicate Detection and Removal
    
    private func removeDuplicateSegments(_ text: String) -> String {
        // –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã, –∞ –Ω–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–ª—è –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
        let paragraphs = text.components(separatedBy: CharacterSet.newlines)
            .map { $0.trimmingCharacters(in: .whitespacesAndNewlines) }
            .filter { !$0.isEmpty && $0.count > 10 } // –ü–æ–≤—ã—à–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–æ 10 —Å–∏–º–≤–æ–ª–æ–≤
        
        print("=== Duplicate Detection Started (Conservative Mode) ===")
        print("Original paragraphs count: \(paragraphs.count)")
        print("Original text length: \(text.count) characters")
        
        var uniqueParagraphs: [String] = []
        var seenParagraphs: [String] = []
        var duplicatesRemoved = 0
        
        for paragraph in paragraphs {
            // Normalize paragraph for comparison (remove extra spaces)
            let normalizedParagraph = paragraph
                .replacingOccurrences(of: "\\s+", with: " ", options: .regularExpression)
                .trimmingCharacters(in: .whitespacesAndNewlines)
            
            // –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ–º case sensitivity —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
            let comparisonKey = normalizedParagraph.lowercased()
            
            // Skip if we've seen this paragraph before (—Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
            var isDuplicate = false
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            if seenParagraphs.contains(comparisonKey) {
                isDuplicate = true
                duplicatesRemoved += 1
                print("Exact duplicate found: '\(String(normalizedParagraph.prefix(50)))...'")
            }
            
            if !isDuplicate {
                uniqueParagraphs.append(normalizedParagraph)
                seenParagraphs.append(comparisonKey)
            }
        }
        
        // –°–æ–µ–¥–∏–Ω—è–µ–º –∞–±–∑–∞—Ü—ã –æ–±—Ä–∞—Ç–Ω–æ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫
        let result = uniqueParagraphs.joined(separator: "\n\n")
        
        print("Unique paragraphs count: \(uniqueParagraphs.count)")
        print("Duplicates removed: \(duplicatesRemoved)")
        print("Final text length: \(result.count) characters")
        if text.count > 0 {
            print("Compression ratio: \(String(format: "%.1f", Double(text.count) / Double(result.count)))x")
        }
        print("=== Duplicate Detection Completed ===")
        
        return result
    }
    
    private func calculateSimilarity(_ text1: String, _ text2: String) -> Double {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        
        // 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if text1 == text2 {
            return 1.0
        }
        
        // 2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã —Å–ª–∏—à–∫–æ–º —Ä–∞–∑–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–µ, –æ–Ω–∏ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        let lengthRatio = Double(min(text1.count, text2.count)) / Double(max(text1.count, text2.count))
        if lengthRatio < 0.5 {
            return 0.0
        }
        
        // 3. Jaccard similarity –ø–æ —Å–ª–æ–≤–∞–º
        let words1 = Set(text1.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
        let words2 = Set(text2.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
        
        let intersection = words1.intersection(words2)
        let union = words1.union(words2)
        
        let jaccardSimilarity = union.isEmpty ? 0.0 : Double(intersection.count) / Double(union.count)
        
        // 4. Levenshtein distance –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫ (–¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤)
        let levenshteinSimilarity: Double
        if text1.count <= 100 && text2.count <= 100 {
            let distance = levenshteinDistance(text1, text2)
            let maxLength = max(text1.count, text2.count)
            levenshteinSimilarity = maxLength > 0 ? 1.0 - Double(distance) / Double(maxLength) : 1.0
        } else {
            levenshteinSimilarity = jaccardSimilarity // Fallback –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        }
        
        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤
        return max(jaccardSimilarity, levenshteinSimilarity)
    }
    
    private func levenshteinDistance(_ str1: String, _ str2: String) -> Int {
        let len1 = str1.count
        let len2 = str2.count
        
        if len1 == 0 { return len2 }
        if len2 == 0 { return len1 }
        
        let str1Array = Array(str1)
        let str2Array = Array(str2)
        
        var dp = Array(repeating: Array(repeating: 0, count: len2 + 1), count: len1 + 1)
        
        for i in 0...len1 {
            dp[i][0] = i
        }
        
        for j in 0...len2 {
            dp[0][j] = j
        }
        
        for i in 1...len1 {
            for j in 1...len2 {
                if str1Array[i - 1] == str2Array[j - 1] {
                    dp[i][j] = dp[i - 1][j - 1]
                } else {
                    dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])
                }
            }
        }
        
        return dp[len1][len2]
    }
    
    // MARK: - Helper Functions
    
    private func estimateTokenCount(_ text: String) -> Int {
        // –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤:
        // - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π/–ª–∞—Ç–∏–Ω—Å–∫–∏–µ —è–∑—ã–∫–∏: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —è–∑—ã–∫–∏ (—Ä—É—Å—Å–∫–∏–π, —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π, –±–æ–ª–≥–∞—Ä—Å–∫–∏–π): ~6-7 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - –Ø–∑—ã–∫–∏ —Å –∏–µ—Ä–æ–≥–ª–∏—Ñ–∞–º–∏ (–∫–∏—Ç–∞–π—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π): ~2-3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - –ê—Ä–∞–±—Å–∫–∏–π/–∏–≤—Ä–∏—Ç: ~5-6 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω
        // - –î—Ä—É–≥–∏–µ —è–∑—ã–∫–∏: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        let words = text.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty }
        let avgCharactersPerToken: Double
        
        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —è–∑—ã–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞
        let sampleText = words.prefix(20).joined() // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É (—Ä—É—Å—Å–∫–∏–π, —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π, –±–æ–ª–≥–∞—Ä—Å–∫–∏–π, —Å–µ—Ä–±—Å–∫–∏–π –∏ —Ç.–¥.)
        let cyrillicRange = sampleText.range(of: "[–∞-—è—ë—ñ]", options: [.regularExpression, .caseInsensitive])
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–µ/—è–ø–æ–Ω—Å–∫–∏–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã
        let cjkRange = sampleText.range(of: "[\\u4e00-\\u9fff\\u3040-\\u309f\\u30a0-\\u30ff]", options: [.regularExpression])
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–∞–±—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        let arabicRange = sampleText.range(of: "[\\u0600-\\u06ff\\u0750-\\u077f]", options: [.regularExpression])
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–≤—Ä–∏—Ç
        let hebrewRange = sampleText.range(of: "[\\u0590-\\u05ff]", options: [.regularExpression])
        
        if cyrillicRange != nil {
            // –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —è–∑—ã–∫–∏ - –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            avgCharactersPerToken = 6.5
        } else if cjkRange != nil {
            // –ö–∏—Ç–∞–π—Å–∫–∏–π/—è–ø–æ–Ω—Å–∫–∏–π - –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            avgCharactersPerToken = 2.5
        } else if arabicRange != nil || hebrewRange != nil {
            // –ê—Ä–∞–±—Å–∫–∏–π/–∏–≤—Ä–∏—Ç - —É–º–µ—Ä–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            avgCharactersPerToken = 5.5
        } else {
            // –ê–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –¥—Ä—É–≥–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ —è–∑—ã–∫–∏
            avgCharactersPerToken = 4.0
        }
        
        let estimatedTokens = Double(text.count) / avgCharactersPerToken
        
        // –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫)
        let punctuationCount = text.filter { ".,!?;:".contains($0) }.count
        let newlineCount = text.filter { $0.isNewline }.count
        
        return Int(ceil(estimatedTokens + Double(punctuationCount) * 0.2 + Double(newlineCount) * 0.5))
    }
    
    private func estimateTokenCountForHistory(_ history: [(String, String)]) -> Int {
        let totalText = history.flatMap { [$0.0, $0.1] }.joined(separator: " ")
        return estimateTokenCount(totalText)
    }
    
    func logModelLimits() {
        let contextLimit = getContextWindowLimit(for: currentModel)
        let outputLimit = getMaxOutputTokens(for: currentModel)
        let chunkSize = getMaxChunkSize(for: currentModel)
        let ratio = Double(contextLimit) / Double(outputLimit)
        
        print("=== Model Limits for \(currentModel.displayName) ===")
        print("Provider: \(currentModel.provider.displayName)")
        print("Context window limit: \(contextLimit) tokens")
        print("Max output tokens: \(outputLimit) tokens")
        print("Max chunk size: \(chunkSize) characters")
        print("Context/Output ratio: \(String(format: "%.1f", ratio))x")
        
        switch currentModel {
        case .gemini2Flash:
            print("‚ÑπÔ∏è  Google AI model with very large context window - excellent for multi-language processing")
        case .glm4Flash:
            print("‚ö†Ô∏è  ZhipuAI model with smaller context window - optimal chunking applied for any language")
        }
        print("=====================================")
    }
}

struct SingleTranscriptionView: View {
    let engine: String
    let transcription: String
    let onDelete: () -> Void
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            // –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–≤–∏–∂–∫–∞ —Å –∫–Ω–æ–ø–∫–∞–º–∏
            HStack {
                HStack {
                    Image(systemName: "brain.head.profile")
                        .foregroundColor(.accentColor)
                    Text(engine.capitalized)
                        .font(.headline)
                        .fontWeight(.semibold)
                    
                    // –°—á–µ—Ç—á–∏–∫ —Å–∏–º–≤–æ–ª–æ–≤
                    Text("(\(transcription.count) chars)")
                        .font(.caption)
                        .foregroundColor(.secondary)
                        .padding(.leading, 4)
                }
                
                Spacer()
                
                // –ö–Ω–æ–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è
                Button(action: onDelete) {
                    Image(systemName: "xmark")
                        .font(.system(size: 14, weight: .medium))
                        .foregroundColor(.secondary)
                        .padding(8)
                        .background(Color(NSColor.controlBackgroundColor))
                        .clipShape(Circle())
                }
                .buttonStyle(PlainButtonStyle())
                .help("Delete \(engine) transcription")
            }
            
            Divider()
            
            // –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è)
            ScrollView {
                Text(transcription)
                    .font(.body)
                    .multilineTextAlignment(.leading)
                    .textSelection(.enabled)
                    .padding(8)
                    .frame(maxWidth: .infinity, alignment: .leading)
                    .background(Color(NSColor.textBackgroundColor))
                    .cornerRadius(8)
            }
            .frame(maxHeight: 400)
        }
        .padding()
        .background(Color(NSColor.controlBackgroundColor))
        .cornerRadius(12)
        .overlay(
            RoundedRectangle(cornerRadius: 12)
                .stroke(Color(NSColor.separatorColor), lineWidth: 1)
        )
    }
}

#Preview {
    TranscriptionsView(audioCaptureService: AudioCaptureService())
}
