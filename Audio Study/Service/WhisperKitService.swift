import Foundation
import AVFoundation
import Combine
import WhisperKit // Uncomment when WhisperKit package is properly added
enum WhisperKitError: Error, LocalizedError {
    case modelNotLoaded
    case audioProcessingFailed
    case transcriptionFailed(String)
    case invalidAudioData
    case modelDownloadFailed
    case whisperKitInitFailed
    case genericError(String)
    
    var localizedDescription: String {
        switch self {
        case .modelNotLoaded:
            return "Whisper model is not loaded. Please wait for model initialization."
        case .audioProcessingFailed:
            return "Failed to process audio data for transcription."
        case .transcriptionFailed(let error):
            return "Transcription failed: \(error)"
        case .invalidAudioData:
            return "Invalid audio data format."
        case .modelDownloadFailed:
            return "Failed to download Whisper model."
        case .whisperKitInitFailed:
            return "Failed to initialize WhisperKit."
        case .genericError(let message):
            return message
        }
    }
}

@MainActor
class WhisperKitService: ObservableObject {
    @Published var isAvailable: Bool = false
    @Published var isProcessing: Bool = false
    @Published var modelLoadingProgress: Double = 0.0
    @Published var isModelLoaded: Bool = false
    @Published var modelLoadingStatus: String = "Initializing..."
    
    // Callbacks for external observation
    var onError: ((Error) -> Void)?
    var onAvailabilityChange: ((Bool) -> Void)?
    var onRecognitionResult: ((String) -> Void)?
    
    private var audioBuffers: [AVAudioPCMBuffer] = []
    private var audioFormat: AVAudioFormat?
    private var isRecording: Bool = false
    private var transcriptionTimer: Timer?
    private let bufferLock = NSLock()
    
    // Accumulated transcription text (like Apple Speech Recognition)
    private var accumulatedText: String = ""
    private var lastProcessedBufferCount: Int = 0
    private var lastTranscriptionLength: Int = 0  // Track length of last transcription to detect new content
    private var lastContextTranscription: String = ""  // Store last context transcription for comparison
    private var lastBufferResetTime: Date = Date()  // –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–±—Ä–æ—Å–∞ –∏–ª–∏ –æ—á–∏—Å—Ç–∫–∏ –±—É—Ñ–µ—Ä–∞
    
    @Published var transcriptionList: [TranscriptionEntry] = [] // Updated type
    
    // WhisperKit instance (will be uncommented when package is added)
    private var whisperKit: WhisperKit?
    // Removed historySeparator

    // Configuration
    private var modelName: String
    @Published var transcriptionInterval: TimeInterval
    @Published var maxBufferDuration: TimeInterval
    
    init(modelName: String = "base",
         transcriptionInterval: TimeInterval = 15.0,  // –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 15 —Å–µ–∫—É–Ω–¥ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
         maxBufferDuration: TimeInterval = 120.0) {   // –£–≤–µ–ª–∏—á–∏–ª–∏ –±—É—Ñ–µ—Ä –¥–æ 2 –º–∏–Ω—É—Ç
        self.modelName = modelName
        self.transcriptionInterval = transcriptionInterval
        self.maxBufferDuration = maxBufferDuration
        
        initializeWhisperKit()
        loadWhisperHistoryFromJSON() // Load history
    }

    private func getDocumentsDirectory() -> URL {
        FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
    }

    private func saveWhisperHistoryToJSON() {
        let fileURL = getDocumentsDirectory().appendingPathComponent("whisper_history.json")
        let encoder = JSONEncoder()
        encoder.outputFormatting = .prettyPrinted

        do {
            let data = try encoder.encode(self.transcriptionList)
            try data.write(to: fileURL, options: [.atomicWrite])
            print("Successfully saved Whisper transcription history to \(fileURL.path)")
        } catch {
            print("Error saving Whisper transcription history to JSON: \(error.localizedDescription)")
        }
    }

    private func loadWhisperHistoryFromJSON() {
        let fileURL = getDocumentsDirectory().appendingPathComponent("whisper_history.json")
        
        guard FileManager.default.fileExists(atPath: fileURL.path) else {
            print("Whisper history JSON file does not exist. Starting with empty history.")
            return
        }

        do {
            let data = try Data(contentsOf: fileURL)
            let decoder = JSONDecoder()
            self.transcriptionList = try decoder.decode([TranscriptionEntry].self, from: data)
            print("Successfully loaded Whisper transcription history from JSON. Count: \(self.transcriptionList.count)")
        } catch {
            print("Error loading Whisper transcription history from JSON: \(error.localizedDescription). Starting with empty history.")
            self.transcriptionList = [] // Ensure clean state on error
        }
    }
    
    // Methods to update configuration
    func updateTranscriptionInterval(_ interval: TimeInterval) {
        guard !isRecording else { return }
        self.transcriptionInterval = interval
        restartPeriodicTranscriptionIfNeeded()
    }
    
    func updateMaxBufferDuration(_ duration: TimeInterval) {
        guard !isRecording else { return }
        self.maxBufferDuration = duration
    }
    
    private func initializeWhisperKit() {
        Task {
            await loadWhisperModel()
        }
    }
    
    private func loadWhisperModel() async {
        print("üöÄ Starting WhisperKit model loading for: \(modelName)")
        
        // Show where models will be stored
        let documentsPath = URL.documentsDirectory.path
        print("üìÅ Models will be stored in: \(documentsPath)")
        
        // Check if model directory already exists
        let modelPath = URL.documentsDirectory.appendingPathComponent("whisperkit-coreml").appendingPathComponent("openai_whisper-\(modelName)")
        let modelExists = FileManager.default.fileExists(atPath: modelPath.path)
        print("üîç Model cache check: \(modelExists ? "EXISTS" : "NOT FOUND") at \(modelPath.path)")
        
        self.modelLoadingProgress = 0.0
        self.modelLoadingStatus = "Starting download..."
        
        // Real WhisperKit implementation:
        do {
            if modelExists {
                print("üíæ Loading cached model: \(modelName)")
                self.modelLoadingStatus = "Loading cached model..."
            } else {
                print("ÔøΩ Downloading model: \(modelName) (this may take a while)")
                self.modelLoadingStatus = "Downloading model (first time)..."
            }
            
            // Try to initialize WhisperKit with local model first
            do {
                whisperKit = try await WhisperKit(
                    model: modelName,
                    downloadBase: URL.documentsDirectory,
                    prewarm: false,
                    load: true,
                    download: false  // Try without download first
                )
                print("‚úÖ Local model loaded successfully from cache")
            } catch {
                print("‚ö†Ô∏è Local model not found, downloading: \(error)")
                // If local fails, try downloading
                whisperKit = try await WhisperKit(
                    model: modelName,
                    downloadBase: URL.documentsDirectory,
                    prewarm: false,
                    load: true,
                    download: true  // Download if local not available
                )
                print("‚úÖ Model downloaded and loaded successfully")
                print("üíæ Model cached for future use at: \(modelPath.path)")
            }
            
            self.modelLoadingProgress = 0.95
            
            // Small delay to show progress
            try? await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
            
            self.isModelLoaded = true
            self.isAvailable = true
            self.modelLoadingProgress = 1.0
            self.onAvailabilityChange?(true)
            print("üéâ WhisperKit initialized successfully with model: \(self.modelName)")
            
        } catch {
            print("‚ùå Failed to initialize WhisperKit: \(error)")
            print("Error details: \(error.localizedDescription)")
            
            self.onError?(WhisperKitError.whisperKitInitFailed)
            self.isAvailable = false
            self.modelLoadingProgress = 0.0
            self.onAvailabilityChange?(false)
        }
        
    }
    
    
    func startRecognition(audioFormat: AVAudioFormat) throws {
        guard isAvailable && isModelLoaded else {
            throw WhisperKitError.modelNotLoaded
        }
        
        self.audioFormat = audioFormat
        self.isRecording = true
        
        bufferLock.lock()
        audioBuffers.removeAll()
        bufferLock.unlock()
        
        // Clear accumulated text for new session
        accumulatedText = ""
        lastProcessedBufferCount = 0
        lastTranscriptionLength = 0
        lastContextTranscription = ""
        lastBufferResetTime = Date() // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º—è —Å–±—Ä–æ—Å–∞ –±—É—Ñ–µ—Ä–∞
        
        // Start periodic transcription
        startPeriodicTranscription()
        
        print("WhisperKit recognition started")
    }
    
    func appendAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard isRecording else { return }
        
        bufferLock.lock()
        defer { bufferLock.unlock() }
        
        audioBuffers.append(buffer)
        
        // Remove old buffers if we exceed max duration
        let totalDuration = audioBuffers.reduce(0.0) { total, buffer in
            return total + Double(buffer.frameLength) / buffer.format.sampleRate
        }
        
        var removedCount = 0
        while totalDuration > maxBufferDuration && !audioBuffers.isEmpty {
            audioBuffers.removeFirst()
            removedCount += 1
        }
        
        // Adjust the processed buffer count when we remove old buffers
        if removedCount > 0 {
            lastProcessedBufferCount = max(0, lastProcessedBufferCount - removedCount)
            // –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è —Å–±—Ä–æ—Å–∞ –±—É—Ñ–µ—Ä–∞, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã
            lastBufferResetTime = Date()
            print("üßπ Buffer reset: removed \(removedCount) old buffers")
        }
    }
    
    func stopRecognition() {
        isRecording = false
        transcriptionTimer?.invalidate()
        transcriptionTimer = nil
        
        bufferLock.lock()
        audioBuffers.removeAll()
        bufferLock.unlock()
        
        // Clear accumulated text when stopping
        accumulatedText = ""
        lastProcessedBufferCount = 0
        lastTranscriptionLength = 0
        lastContextTranscription = ""
        lastBufferResetTime = Date() // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –±—É—Ñ–µ—Ä–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
                
        print("WhisperKit recognition stopped")
    }
    
    private func startPeriodicTranscription() {
        // Cancel existing timer if any
        transcriptionTimer?.invalidate()
        transcriptionTimer = Timer.scheduledTimer(withTimeInterval: transcriptionInterval, repeats: true) { [weak self] _ in
            Task { @MainActor in
                self?.performTranscription()
            }
        }
    }
    
    // Restart timer with new interval if recording
    private func restartPeriodicTranscriptionIfNeeded() {
        if isRecording {
            startPeriodicTranscription()
        }
    }
    
    private func performTranscription() {
        guard isRecording && !audioBuffers.isEmpty else { return }
        
        bufferLock.lock()
        let allBuffers = Array(audioBuffers)
        bufferLock.unlock()
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        // –ë–µ—Ä–µ–º –≤–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞
        let contextDuration: TimeInterval = maxBufferDuration
        let totalDuration = allBuffers.reduce(0.0) { total, buffer in
            return total + Double(buffer.frameLength) / buffer.format.sampleRate
        }
        
        let buffersToProcess: [AVAudioPCMBuffer]
        if totalDuration <= contextDuration {
            // –ï—Å–ª–∏ –æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ
            buffersToProcess = allBuffers
            print("üîÑ Processing entire session: \(String(format: "%.1f", totalDuration))s (\(allBuffers.count) buffers)")
        } else {
            // –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ buffers –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö maxBufferDuration –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            var accumulatedDuration: TimeInterval = 0
            var startIndex = allBuffers.count
            
            for i in stride(from: allBuffers.count - 1, through: 0, by: -1) {
                let buffer = allBuffers[i]
                let bufferDuration = Double(buffer.frameLength) / buffer.format.sampleRate
                accumulatedDuration += bufferDuration
                startIndex = i
                
                if accumulatedDuration >= contextDuration {
                    break
                }
            }
            
            buffersToProcess = Array(allBuffers[startIndex...])
            print("üîÑ Processing with full context: \(String(format: "%.1f", accumulatedDuration))s (\(buffersToProcess.count) buffers)")
        }
        
        Task {
            await transcribeWithContext(buffersToProcess)
        }
    }
    
    private func transcribeWithContext(_ buffers: [AVAudioPCMBuffer]) async {
        guard let audioFormat = audioFormat else { return }
        
        self.isProcessing = true
        
        do {
            // Combine all buffers into a single audio array
            let audioData = try combineBuffersToFloatArray(buffers, format: audioFormat)
            
            // Add audio diagnostics
            let duration = Double(audioData.count) / audioFormat.sampleRate
            let rms = sqrt(audioData.map { $0 * $0 }.reduce(0, +) / Float(audioData.count))
            print("üéµ Context audio diagnostics: \(String(format: "%.2f", duration))s duration, RMS: \(String(format: "%.4f", rms))")
            
            // Skip transcription if audio is too short or quiet
            guard duration >= 0.5 && rms > 0.001 else {
                print("‚ö†Ô∏è Skipping transcription: audio too short or quiet")
                self.isProcessing = false
                return
            }
            
            // Transcribe using WhisperKit
            let transcription = try await transcribeAudioData(audioData)
            
            // Update UI on main thread - use smart transcription replacement
            self.isProcessing = false
            if !transcription.isEmpty {
                // Use the current transcription as the new accumulated text
                // WhisperKit's context windows provide the best available transcription
                accumulatedText = transcription
                
                // Get clean versions of transcriptions for comparison
                let cleanTranscription = transcription.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
                
                let timeSinceBufferReset = Date().timeIntervalSince(lastBufferResetTime)
                // preventing random sudden changes as much as possible. Saving model only at the last moment
                let canAddTranscription = timeSinceBufferReset > maxBufferDuration * 0.90
                print("Buffer reset time: \(String(format: "%.1f", timeSinceBufferReset))s, max buffer duration: \(String(format: "%.1f", maxBufferDuration))s")
       
                if canAddTranscription {
                    if !self.transcriptionList.isEmpty {
                        let lastIndex = self.transcriptionList.count - 1
                        // Assuming lastTranscription here is used for its textual content for similarity
                        let lastTranscriptionText = self.transcriptionList[lastIndex].transcription 
                        let cleanLastTranscription = lastTranscriptionText.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
                        
                        let similarity = calculateTextSimilarity(cleanTranscription, cleanLastTranscription)
                        print("üîç Similarity: \(String(format: "%.2f", similarity))")
                        if similarity > 0.5 {
                            let updatedEntry = TranscriptionEntry(date: Date(), transcription: transcription)
                            self.transcriptionList[lastIndex] = updatedEntry
                            self.saveWhisperHistoryToJSON()
                            print("üîÑ Updated last transcription entry with more complete version: \(transcription.count) chars")
                        } else {
                            let newEntry = TranscriptionEntry(date: Date(), transcription: accumulatedText) // accumulatedText contains the full new transcription
                            self.transcriptionList.append(newEntry)
                            self.saveWhisperHistoryToJSON()
                        }
                    } else {
                        // –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å
                        let newEntry = TranscriptionEntry(date: Date(), transcription: accumulatedText)
                        self.transcriptionList.append(newEntry)
                        self.saveWhisperHistoryToJSON()
                        print("‚ûï Added first transcription entry to list: \(accumulatedText.count) chars")
                    }
                }
                print("üéµ Current transcription (accumulatedText): \(accumulatedText.count) chars")
                self.onRecognitionResult?(accumulatedText)
            }
            
        } catch {
            self.isProcessing = false
            self.onError?(error)
        }
    }
    
    
    private func transcribeBuffers(_ buffers: [AVAudioPCMBuffer]) async {
        guard let audioFormat = audioFormat else { return }
        
        self.isProcessing = true
        
        do {
            // Combine all buffers into a single audio array
            let audioData = try combineBuffersToFloatArray(buffers, format: audioFormat)
            
            // Add audio diagnostics
            let duration = Double(audioData.count) / audioFormat.sampleRate
            let rms = sqrt(audioData.map { $0 * $0 }.reduce(0, +) / Float(audioData.count))
            print("üîç Audio diagnostics: \(String(format: "%.2f", duration))s duration, RMS: \(String(format: "%.4f", rms))")
            
            // Skip transcription if audio is too quiet or too short
            guard duration >= 0.5 && rms > 0.001 else {
                print("‚ö†Ô∏è Skipping transcription: audio too short or quiet")
                self.isProcessing = false
                return
            }
            
            // Transcribe using WhisperKit
            let transcription = try await transcribeAudioData(audioData)
            
            // Update UI on main thread
            self.isProcessing = false
            if !transcription.isEmpty {
                // –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É (–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è)
                if !accumulatedText.isEmpty {
                    accumulatedText += " " + transcription
                } else {
                    accumulatedText = transcription
                }
                
                // Send the accumulated transcription
                self.onRecognitionResult?(accumulatedText)
                print("WhisperKit added text: \(transcription)")
                print("WhisperKit total text: \(accumulatedText)")
            }
            
        } catch {
            self.isProcessing = false
            self.onError?(error)
        }
    }
    
    private func combineBuffersToFloatArray(_ buffers: [AVAudioPCMBuffer], format: AVAudioFormat) throws -> [Float] {
        let totalFrames = buffers.reduce(0) { $0 + Int($1.frameLength) }
        var audioData: [Float] = []
        audioData.reserveCapacity(totalFrames)
        
        for buffer in buffers {
            let frameLength = Int(buffer.frameLength)
            guard let channelData = buffer.floatChannelData?[0] else {
                throw WhisperKitError.invalidAudioData
            }
            
            for i in 0..<frameLength {
                audioData.append(channelData[i])
            }
        }
        
        // Normalize audio data to prevent clipping and improve recognition
        let normalizedAudio = normalizeAudio(audioData)
        
        print("üéµ Combined \(buffers.count) buffers into \(normalizedAudio.count) samples")
        return normalizedAudio
    }
    
    private func normalizeAudio(_ audioData: [Float]) -> [Float] {
        guard !audioData.isEmpty else { return audioData }
        
        // Find the maximum absolute value
        let maxValue = audioData.map { abs($0) }.max() ?? 1.0
        
        // Avoid division by zero and don't normalize if already quiet
        guard maxValue > 0.001 else { return audioData }
        
        // Normalize to 0.8 to prevent clipping
        let normalizationFactor = 0.8 / maxValue
        return audioData.map { $0 * normalizationFactor }
    }
    
    private func transcribeAudioData(_ audioData: [Float]) async throws -> String {
        // Real WhisperKit implementation:
        guard let whisperKit = whisperKit else {
            throw WhisperKitError.modelNotLoaded
        }
        
        do {
            // WhisperKit expects 16kHz sample rate
            let targetSampleRate: Float = 16000.0
            let currentSampleRate = Float(audioFormat?.sampleRate ?? 44100.0)
            
            // Resample audio if needed
            let processedAudio: [Float]
            if currentSampleRate != targetSampleRate {
                processedAudio = resampleAudio(audioData, from: currentSampleRate, to: targetSampleRate)
                print("üîÑ Resampled audio from \(currentSampleRate)Hz to \(targetSampleRate)Hz")
            } else {
                processedAudio = audioData
            }
            
            // Ensure minimum audio length (WhisperKit needs at least ~1 second)
            guard processedAudio.count >= Int(targetSampleRate * 0.5) else {
                print("‚ö†Ô∏è Audio too short: \(processedAudio.count) samples")
                return ""
            }
            
            print("üéµ Processing audio: \(processedAudio.count) samples at \(targetSampleRate)Hz")
            
            // Transcribe using WhisperKit with audioArray method
            let results = try await whisperKit.transcribe(audioArray: processedAudio, decodeOptions: DecodingOptions(task: .transcribe, language: "en"))
            
            // Extract text from first result (WhisperKit returns [TranscriptionResult])
            let transcription = results.first?.text.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines) ?? ""
            
            print("‚úÖ WhisperKit transcription: \(transcription)")
            return transcription
            
        } catch {
            print("‚ùå WhisperKit transcription error: \(error)")
            throw WhisperKitError.transcriptionFailed(error.localizedDescription)
        }
        
    }
    
    private func resampleAudio(_ audioData: [Float], from sourceSampleRate: Float, to targetSampleRate: Float) -> [Float] {
        let ratio = sourceSampleRate / targetSampleRate
        let targetLength = Int(Float(audioData.count) / ratio)
        var resampledData: [Float] = []
        resampledData.reserveCapacity(targetLength)
        
        for i in 0..<targetLength {
            let sourceIndex = Int(Float(i) * ratio)
            if sourceIndex < audioData.count {
                resampledData.append(audioData[sourceIndex])
            }
        }
        
        return resampledData
    }
    
    
    
}

// MARK: - WhisperKit Integration Helper
extension WhisperKitService {
    
    /// Returns available Whisper models
    static func availableModels() -> [String] {
        return [
            "tiny",
            "base",
            "small",
            "medium",
            "large-v2",
            "large-v3"
        ]
    }
    
    /// Returns model size information
    static func modelInfo(for modelName: String) -> (size: String, description: String) {
        switch modelName {
        case "tiny":
            return ("150-200 MB", "Fastest, least accurate")
        case "base":
            return ("290-350 MB", "Good balance of speed and accuracy")
        case "small":
            return ("970 MB - 1.2 GB", "Better accuracy, slower")
        case "medium":
            return ("2.3 - 2.8 GB", "High accuracy, much slower")
        case "large-v2", "large-v3":
            return ("4.5 - 5.5 GB", "Best accuracy, very slow")
        default:
            return ("Unknown", "Custom model")
        }
    }
    
    /// Switch to a different model
    func switchModel(to newModelName: String) {
        guard !isRecording else {
            onError?(WhisperKitError.genericError("Cannot switch models while recording"))
            return
        }
        
        // Update the model name
        self.modelName = newModelName
        
        // Reset state
        self.whisperKit = nil
        self.isModelLoaded = false
        self.isAvailable = false
        self.modelLoadingProgress = 0.0
        self.modelLoadingStatus = "Switching to \(newModelName)..."
        
        // Keep accumulated text when switching models
        
        // Load new model
        Task {
            await loadWhisperModel()
        }
    }
    
    // –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
    private func calculateTextSimilarity(_ text1: String, _ text2: String) -> Double {
        // –ï—Å–ª–∏ –æ–¥–∏–Ω –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
        if text1.isEmpty || text2.isEmpty {
            return 0.0
        }
        
        // –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 1
        if text1 == text2 {
            return 1.0
        }
        
        // –ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –¥—Ä—É–≥–æ–º
        if text1.count > text2.count {
            if text1.contains(text2) {
                return Double(text2.count) / Double(text1.count)
            }
        } else {
            if text2.contains(text1) {
                return Double(text1.count) / Double(text2.count)
            }
        }
        
        // –ë–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å—á–∏—Ç–∞–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        let words1 = Set(text1.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
        let words2 = Set(text2.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
        
        if words1.isEmpty || words2.isEmpty {
            return 0.1
        }
        
        let commonWords = words1.intersection(words2)
        let similarity = Double(commonWords.count) / Double(max(words1.count, words2.count))
        
        return similarity
    }
}

// MARK: - AVAudioEngine Error Handling
extension AVAudioEngine {
    /// Safely starts the audio engine with proper error handling for HAL errors
    func safeStart() throws {
        do {
            try self.start()
        } catch {
            // Handle specific CoreAudio errors
            let nsError = error as NSError
            if nsError.domain == NSOSStatusErrorDomain {
                // Common CoreAudio error codes
                switch nsError.code {
                case -10877: // kAudioHardwareNotRunningError
                    print("üîä Audio Hardware Error: Device not available or running (-10877)")
                    throw AppError.deviceNotAvailable
                    
                case -10875: // kAudioHardwareUnspecifiedError
                    print("üîä Audio Hardware Error: Unspecified hardware error (-10875)")
                    throw AppError.coreAudioError(nsError.code, "Hardware error")
                    
                case -10851: // kAudioHardwareUnsupportedOperationError
                    print("üîä Audio Hardware Error: Unsupported operation (-10851)")
                    throw AppError.coreAudioError(nsError.code, "Unsupported operation")
                    
                default:
                    print("üîä Audio Hardware Error: \(nsError.code)")
                    throw AppError.coreAudioError(nsError.code, "Unknown audio error")
                }
            } else {
                throw error
            }
        }
    }
}

// Add comprehensive CoreAudio error code mapping
extension AppError {
    static func mapCoreAudioErrorCode(_ code: Int) -> String {
        switch code {
        case -10877:
            return "Audio device not available or not running. Check that your audio device is properly connected and that no other app is using it exclusively."
        case -10875:
            return "Unspecified audio hardware error. Try restarting your Mac or reconnecting your audio devices."
        case -10851:
            return "Unsupported audio operation. Your current audio configuration may not be compatible."
        case -10879:
            return "Audio device not found. Check your audio hardware connections."
        case -10878:
            return "Audio device is busy. Another application may be using the audio device exclusively."
        case -10876:
            return "Audio hardware initialization failed."
        case -10868:
            return "Audio hardware stream format error."
        case -10867:
            return "Audio hardware is in use by another application."
        case -10866:
            return "Audio hardware IO operation aborted."
        default:
            return "CoreAudio error \(code). Check your audio hardware and settings."
        }
    }
}
